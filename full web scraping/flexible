import requests
from bs4 import BeautifulSoup
import csv

def auto_scrape(url, fields, output_file):
    """
    Automatically scrapes a web page for specified fields.
    
    Parameters:
    - url: The website URL
    - fields: Dictionary of field_name: HTML tag (optionally with class), e.g.,
        {"Quote": ("span", "text"), "Author": ("small", "author"), "Tags": ("a", "tag")}
    - output_file: CSV filename to save data
    """
    response = requests.get(url)
    if response.status_code != 200:
        print("Failed to fetch the webpage")
        return

    soup = BeautifulSoup(response.text, "html.parser")

    # Assume the main items are in a repeated container (div or section)
    # We'll take the parent of the first field as container
    first_field_name = list(fields.keys())[0]
    tag, class_name = fields[first_field_name]
    first_elements = soup.find_all(tag, class_=class_name)
    
    if not first_elements:
        print("No elements found with the given first field selector.")
        return

    # Get the container for each item
    containers = [el.parent for el in first_elements]

    all_data = []
    for container in containers:
        row = []
        for field_name, (tag, class_name) in fields.items():
            elements = container.find_all(tag, class_=class_name)
            if elements:
                if len(elements) > 1:
                    row.append(", ".join([el.text.strip() for el in elements]))
                else:
                    row.append(elements[0].text.strip())
            else:
                row.append("")
        all_data.append(row)

    # Save to CSV
    with open(output_file, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(fields.keys())
        writer.writerows(all_data)

    print(f"Scraping completed! Total items: {len(all_data)}")
    print(f"Saved to {output_file}")


# --- Example Usage ---
if __name__ == "__main__":
    url = "http://quotes.toscrape.com/page/1/"
    fields = {
        "Quote": ("span", "text"),
        "Author": ("small", "author"),
        "Tags": ("a", "tag")
    }
    output_file = "auto_quotes.csv"

    auto_scrape(url, fields, output_file)
